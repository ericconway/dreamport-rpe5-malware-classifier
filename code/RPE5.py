"""
Usage:

# install dependencies
pip install sklearn
sudo apt-get install clamav
freshclam  # follow instruction to comment out "example" and rename file.
freshclam  # this time it will download signatures
git clone https://github.com/radare/radare2.git
radare2/sys/install.sh

# help
python RPE5.py --help
python RPE5.py ingest --help
python RPE5.py learn --help
python RPE5.py classify --help

# extract features
python RPE5.py ingest /corpus/family_1/ family:family_1,software:malware --concurrency 8
python RPE5.py ingest /corpus/family_2/ family:family_2,software:malware --concurrency 8

# if you only have extracted json from our repo and not the actual malware binaries:
python RPE5.py ingest path/to/json/data/features/ none --concurrency 8

# train models on extracted features
python RPE5.py learn

# classify unknown files
python RPE5.py classify /corpus/unknown/

# if you only have extracted json from our repo and not the actual malware binaries:
python RPE5.py classify path/to/json/data/features/

# testing (use subset of ingested files for training, use the balance for classification)
python RPE5.py learn --train-percent 70;python RPE5.py classify /corpus --train-percent 70

"""

# Usability:
#   TODO review and comment all code
# Features extraction:
#   TODO (!) recurse over clamav temp files
#   TODO string meta: line endings, encoding, indentation none/tab/space/mixed, commenting, keywords for js/html, base64
#   TODO string disassembly: js/html keywords
#   TODO disassembly ngrams, sort and take subset for order invariance?
#   TODO dissasembly capture symbol order
#   TODO improve support for jar, .net mono assembly, zip, javascript, html
#   TODO base64 decoding
#   TODO tune thresholds
#   TODO imphash, pehash, richpe
#   TODO review authorship paper
# ML
#   TODO use predict_proba() to get confidence number
#   TODO change indentify.py into a classifier.  filter out common and singleton features during learning?
# Other:
#   TODO performance counter, report how long each plugin takes
#   TODO timeout in case plugin gets stuck
# Test:
#   TODO (!) build up corpus with more file formats, benign file types to balance
#   TODO generate diversified files (UPX, replace strings, add junk or other file to end, blank parts of file)
#   TODO test, tune, tweak, hunt for edge cases
# Performance:
#   TODO multiprocessing more efficiently

####################################
# Questions for organizers
#    Roughly how many file should we expect in the competition corpus?
#    Are PCAP files part of this RPE?  If so, how will they be used?
#    How many files are the offensive teams expected to generate?
#    Will any of the offensive team's files be labeled for training, or will they all be unlabeld for classification?
#    Can defensive teams use public internet connected services?
#    Can defensive teams use private internet connected resources?
#    Can defensive teams incorporation commercial and/or open source antivirus products?
#    Is there a scoring difference between mis-classifying a sample file and not classifying it?
#        In other words, is it the same to incorrectly report a file is "family_1" as it is to report the file as "inconclusive"?


import argparse
import collections
import hashlib
import importlib
import json
import os
import pickle
import subprocess
import sys
import time
import traceback
import zlib

#################################################################
# Globals
#################################################################

PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

EXTRACTOR_PLUGIN_DIR  = 'extractors'
IDENTIFIER_PLUGIN_DIR = 'identifiers'
CLASSIFIER_PLUGIN_DIR = 'classifiers'

DATABASE_PATH = os.path.join(PROJECT_ROOT, 'database')

#################################################################
# Code
#################################################################


class Storage(object):

    def __init__(self, root=DATABASE_PATH, read_only=False):
        self._root = root
        self._read_only = read_only
        if not os.path.exists(root):
            os.makedirs(root)

    def store(self, subdirs, name, data, fmt='json'):
        if self._read_only:
            return
        base = os.path.join(self._root, subdirs)
        if not os.path.exists(base):
            os.makedirs(base)
        if fmt == 'json':
            fpath = os.path.join(base, name+'.json')
            with open(fpath, 'wb') as f:
                return f.write(json.dumps(data, indent=4).encode('utf-8', errors='replace'))
        if fmt == 'pickle':
            fpath = os.path.join(base, name+'.pickle')
            with open(fpath, 'wb') as f:
                return f.write(pickle.dumps(data))
        raise ValueError('Storage.store() format must be "json" or "pickle"')

    def load(self, subdirs, name):
        fpath = os.path.join(self._root, subdirs, name)
        if (not fpath.endswith('.json')) and (not fpath.endswith('.pickle')):
            if os.path.exists(fpath + '.json'):
                fpath += '.json'
            elif os.path.exists(fpath + '.pickle'):
                fpath += '.pickle'
        if fpath.endswith('.json'):
            try:
                with open(fpath, 'rb') as f:
                    return json.loads(f.read().decode('utf-8', errors='replace'))
            except (OSError, json.decoder.JSONDecodeError) as e:
                print('Error reading %s.  %s'%(fpath, e))
                return None
        if fpath.endswith('.pickle'):
            with open(fpath, 'rb') as f:
                return pickle.loads(f.read())
        raise FileNotFoundError('Unable to find ' + fpath)

    def load_all(self, subdirs):
        base = os.path.join(self._root, subdirs)
        if os.path.exists(base):
            for name in os.listdir(base):
                if name.endswith('.json') or name.endswith('.pickle'):
                    yield self.load(subdirs, name)

    def self_test(self):
        data1 = [1, 2, 3]
        data2 = [4, 5, 6]
        data3 = [7, 8, 9]
        self.store('test/a', 'file1', data1)
        self.store('test/a', 'file2', data2)
        self.store('test/a', 'file3', data3, 'pickle')
        assert self.load('test/a', 'file1') == data1
        assert self.load('test/a', 'file2') == data2
        assert self.load('test/a', 'file3') == data3
        result = list(self.load_all('test/a'))
        assert data1 in result
        assert data2 in result
        assert data3 in result


def md5sum(filepath):
    """return md5 sum of file"""
    hasher = hashlib.md5()
    with open(filepath, 'rb') as f:
        hasher.update(f.read())
    return hasher.hexdigest()


def filetype(filepath):
    """run host's "file" command to get type"""
    output = subprocess.check_output(['file', filepath])
    output = output.decode('utf-8').split(':', 1)[-1].strip()  # remove file name
    return output


def labels2str(labels_dict):
    """
    convert the labels dictionary into a string.
    in:    {'key1':value1, 'key2':value2}
    out:   "key1:value1,key2:value2"
    """
    out = []
    for k,v in labels_dict.items():
        out.append(k+':'+v)
    out.sort()
    return ','.join(out)


def str2labels(labels_str):
    """
    convert a label string into a dictionary.
    in:    "key1:value1,key2:value2"
    out:   {'key1':value1, 'key2':value2}
    """
    labels_dict = {}
    if labels_str and labels_str.lower() != 'none':
        for entry in labels_str.split(','):
            k,v = entry.split(':')
            k = k.strip()
            v = v.strip()
            labels_dict[k.lower().strip()] = v
    return labels_dict


def init_fileinfo(fileinfo=None, filepath=''):
    """initialize fileinfo dict"""
    if fileinfo is None:
        fileinfo_out = {
            'md5': None,
            'type': None,
        }
    else:
        # make a copy
        fileinfo_out = {}
        for k,v in fileinfo.items():
            fileinfo_out[k] = v
    if filepath:
        if ('md5' not in fileinfo_out) or (not fileinfo_out['md5']):
            fileinfo_out['md5'] = md5sum(filepath)
        if ('type' not in fileinfo_out) or (not fileinfo_out['type']):
            fileinfo_out['type'] = filetype(filepath)
    if 'labels' not in fileinfo_out:
        fileinfo_out['labels'] = {}
    return fileinfo_out


def load_plugins(plugin_dir):
    """return list of plugin modules imported from plugin_dir located in PROJECT_ROOT"""
    plugins = []

    plugin_path = os.path.join(PROJECT_ROOT, plugin_dir)
    # print('loading plugins from '+plugin_path)

    for plugin_py in os.listdir(plugin_path):
        # print(plugin_py)
        if plugin_py.startswith('_'):
            continue
        if not plugin_py.endswith('.py'):
            continue
        plugin_name = plugin_py[0:-3]
        plugin_mod = importlib.import_module('.'.join([plugin_dir, plugin_name]))
        if plugin_mod.__enabled__:
            plugins.append(plugin_mod)

    # print(plugins)
    return plugins


def load_all_plugins():

    all_plugins = {
        'ex_plugins': load_plugins(EXTRACTOR_PLUGIN_DIR),
        'id_plugins': load_plugins(IDENTIFIER_PLUGIN_DIR),
        'class_plugins': load_plugins(CLASSIFIER_PLUGIN_DIR),
    }
    return all_plugins


def run_plugins(plugins, filepath, fileinfo, **kwargs):

    results = {
        'filepath': filepath,
        'fileinfo': fileinfo,
        'plugins': [],
    }
    for plugin in plugins:
        result = None
        try:
            result = plugin.run(filepath, fileinfo, **kwargs)
        except KeyboardInterrupt:
            raise
        except Exception:
            print(traceback.format_exc())
        results['plugins'].append({
            'plugin': plugin.__name__,
            'version': plugin.__version__,
            'results': result,
        })
        # print('Plugin %s had %d results.' % (plugin.__name__, len(result)))
    return results


def extract_features(filepath, fileinfo, storage, ex_plugins, fresh=True):
    print('')
    print(filepath)
    results = None

    if filepath.endswith('.json'):
        # this is a feature extraction file and not a malware sample
        # reuse provided results
        with open(filepath, 'rb') as f:
            results = json.loads(f.read().decode('utf-8', errors='replace'))
        # sanity check file format
        for k in ['filepath', 'fileinfo', 'plugins']:
            if k not in results:
                raise ValueError('Invalid json feature file, expecting "%s"' % k)
        print('    Original file: '+results['filepath'])
        # merge labels
        for k,v in fileinfo['labels'].items():
            results['fileinfo']['labels'][k] = v
        fileinfo = results['fileinfo']
        storage.store('features', fileinfo['md5'], results)

    elif not fresh:
        # reuse existing results if available
        try:
            results = storage.load('features', fileinfo['md5'])
            print('    Reusing existing feature extraction')
        except FileNotFoundError:
            pass

    if results is None:
        # ok, we actually need to extract features
        results = run_plugins(ex_plugins, filepath, fileinfo)
        storage.store('features', fileinfo['md5'], results)
    print('    Info:')
    print('        MD5: '+str(results['fileinfo'].get('md5', 'unknown')))
    print('        Type: '+str(results['fileinfo'].get('type', 'unknown')))
    print('        ClamAV: '+str(results['fileinfo'].get('clamav', 'unknown')))
    print('        Labels: '+str(labels2str(results['fileinfo']['labels'])))
    return results


def identify_file(filepath, fileinfo, feature_data, storage, training, id_plugins, fresh=True):

    if not fresh:
        # return existing results if available
        try:
            results = storage.load('identities/minhashes', fileinfo['md5'])
            print('Reusing existing identification')
            return results
        except FileNotFoundError:
            pass
    results = run_plugins(id_plugins, filepath, fileinfo, feature_data=feature_data, storage=storage, training=training)
    storage.store('identities/minhashes', fileinfo['md5'], results)
    return results


def ingest_file(filepath, fileinfo, storage, training, all_plugins, fresh):
    fileinfo = init_fileinfo(fileinfo, filepath)
    feature_data = extract_features(filepath, fileinfo, storage, all_plugins['ex_plugins'], fresh)
    identity_data = identify_file(filepath, fileinfo, feature_data, storage, training, all_plugins['id_plugins'], fresh)
    return feature_data, identity_data


def ingest(args, storage=None, all_plugins=None):

    # multi-processing
    concurrency = args.concurrency
    thread_id = args.thread_id
    if concurrency > 1 and thread_id is None:
        # spin up subprocess
        processes = []
        for tid in range(concurrency):
            cmd = [sys.executable] + sys.argv + ['--thread-id', str(tid)]
            print(' '.join(cmd))
            processes.append(subprocess.Popen(cmd))
        for proc in processes:
            proc.wait()
        return

    if storage is None:
        storage = Storage(args.database)
    if all_plugins is None:
        all_plugins = load_all_plugins()
    fresh = args.fresh

    fileinfo = {
        'labels': str2labels(args.labels),
    }
    training = True

    if os.path.isfile(args.filepath):
        ingest_file(args.filepath, fileinfo, storage, training, all_plugins)

    elif os.path.isdir(args.filepath):
        for root, dirs, files in os.walk(args.filepath):
            for file in files:
                if file == '.DS_Store':
                    continue  # skip Mac OS file system data

                # for multiprocessing, only process files corresponding with thread_id
                if concurrency > 1:
                    if zlib.crc32(file.encode('utf-8')) % concurrency != thread_id:
                        continue

                filepath = os.path.join(root, file)
                ingest_file(filepath, fileinfo, storage, training, all_plugins, fresh)
    else:
        raise FileNotFoundError('can\'t find '+args.filepath)


def learn(args, storage=None):

    if storage is None:
        storage = Storage(args.database)
    class_plugins = load_plugins(CLASSIFIER_PLUGIN_DIR)

    train_percent = args.train_percent
    rand_seed = args.seed

    results = run_plugins(class_plugins, filepath=None, fileinfo=None, storage=storage, mode='learn',
                          train_percent=train_percent, rand_seed=rand_seed)
    return results


def classify_file(filepath, fileinfo=None, storage=None, all_plugins=None,
                      train_percent=100, rand_seed='0', fresh=True):

    if fileinfo is None:
        fileinfo = init_fileinfo(filepath=filepath)
    if storage is None:
        storage = Storage()
    if all_plugins is None:
        all_plugins = load_all_plugins()

    training = False

    feature_data = extract_features(filepath, fileinfo, storage, all_plugins['ex_plugins'], fresh)

    fileinfo = feature_data['fileinfo']  # fileinfo may get enriched by extraction

    classify_data = run_plugins(all_plugins['class_plugins'], filepath=filepath, fileinfo=fileinfo, storage=storage,
                                mode='classify', file_data=feature_data, train_percent=train_percent, rand_seed=rand_seed)
    identity_data = identify_file(filepath, fileinfo, feature_data, storage, training, all_plugins['id_plugins'])
    return classify_data, identity_data


def classify(args, storage=None, all_plugins=None):
    if storage is None:
        storage = Storage(args.database, read_only=False)
    if all_plugins is None:
        all_plugins = load_all_plugins()

    train_percent = args.train_percent
    rand_seed = args.seed
    fresh = args.fresh

    if os.path.isfile(args.filepath):
        classify_file(args.filepath, fileinfo=None, storage=storage, all_plugins=all_plugins,
                      train_percent=train_percent, rand_seed=rand_seed, fresh=fresh)
    elif os.path.isdir(args.filepath):
        class_counter = collections.Counter()
        success_count = 0
        total_count = 0
        try:
            for root, dirs, files in os.walk(args.filepath):
                for file in files:
                    if file == '.DS_Store':
                        continue  # skip Mac OS file system data
                    filepath = os.path.join(root, file)
                    classify_data,identify_data = classify_file(filepath, fileinfo=None, storage=storage,
                        all_plugins=all_plugins, train_percent=train_percent, rand_seed=rand_seed, fresh=fresh)

                    # compare prediction against any labels we have
                    fileinfo = classify_data['fileinfo']
                    for classify_plugin in classify_data['plugins']:
                        plugin_name = classify_plugin['plugin']
                        for classified_as in classify_plugin['results'].split(','):
                            label_k = classified_as.split(':')[0]
                            label_v = classified_as.split(':')[-1]
                            if 'labels' in fileinfo and label_k in fileinfo['labels']:
                                actual_v = fileinfo['labels'][label_k]
                                if actual_v == label_v:
                                    success_count += 1
                                total_count += 1
                            else:
                                actual_v = '?'
                            actual_classification = label_k+':'+actual_v
                            result_str = plugin_name + ' classified ' + actual_classification + ' as ' + classified_as
                            class_counter[result_str] += 1

        except KeyboardInterrupt:
            print('')
            print('Exit early due to KeyboardInterrupt')
            print('')

        for result_str in class_counter:
            print(result_str + ': '+str(class_counter[result_str]))
        if total_count > 0:
            print('Accuracy: '+str(100*success_count/total_count))
    else:
        raise FileNotFoundError('can\'t find '+args.filepath)


def get_arg_parser():
    parser = argparse.ArgumentParser(description='RPE5 malware feature extractor and classifier')
    parser.add_argument('--database', type=str, help='path for storing all ingested data and derived models',
                        default=DATABASE_PATH)
    subparsers = parser.add_subparsers(help='Commands', dest='command')

    # "ingest" command
    parser_ingest = subparsers.add_parser('ingest', help='Extract features from files')
    parser_ingest.add_argument('filepath', type=str, help='Path to file or directory of files')
    parser_ingest.add_argument('labels', type=str,
                               help='"none" or dictionary of labels, for example "family:family_1,author:fred".  '
                               "You can use any labels you want.",)
    parser_ingest.add_argument('--fresh', action='store_true',
                               help='perform fresh extraction on files that have been previously extracted')
    thread_group = parser_ingest.add_argument_group('Threading')
    thread_group.add_argument('--concurrency', type=int, help='Number of concurrent processes to run', default=1)
    thread_group.add_argument('--thread-id', type=int, help='Use --concurrency instead', default=None)

    # "learn" command
    parser_learn = subparsers.add_parser('learn', help='Build ML models based on previously ingested files')
    test_group = parser_learn.add_argument_group('Testing')
    test_group.add_argument('--train-percent', type=float, help='Percentage of files to learn from', default=100)
    test_group.add_argument('--seed', type=str, help='Random seed for file selection', default='0')

    # "classify" command
    parser_classify = subparsers.add_parser('classify', help='classify files')
    parser_classify.add_argument('filepath', type=str, help='Path to file or directory of files')
    parser_classify.add_argument('--fresh', action='store_true',
                               help='perform fresh extraction on files that have been previously extracted')
    test_group = parser_classify.add_argument_group('Testing')
    test_group.add_argument('--train-percent', type=float, default=100,
                            help='Percentage of files used in learn stage.  Inverse will be used for test.')
    test_group.add_argument('--seed', type=str, help='Random seed for file selection', default='0')

    return parser


def main(argv):
    start_time = time.time()
    parser = get_arg_parser()
    args = parser.parse_args(argv)
    try:
        if args.command == 'ingest':
            ingest(args)
        elif args.command == 'learn':
            learn(args)
        elif args.command == 'classify':
            classify(args)
        else:
            print('Unknown command')
            parser.print_usage()
            exit(-1)
    except Exception as e:
        raise e
    finally:
        print("run time: %0.1f seconds" % (time.time() - start_time))

    try:
        thread_id = args.thread_id
    except AttributeError:
        thread_id = None
    if thread_id is not None:
        print("done thread %d." % args.thread_id)
    else:
        print("done.")


if __name__ == '__main__':
    main(sys.argv[1:])


