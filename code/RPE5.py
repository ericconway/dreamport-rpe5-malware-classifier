"""
Usage:

# install dependencies
pip install sklearn
sudo apt-get install clamav
sudo apt-get install p7zip-full
freshclam  # follow instruction to comment out "example" and rename file.
freshclam  # this time it will download signatures
git clone https://github.com/radare/radare2.git
radare2/sys/install.sh

# help
python RPE5.py --help
python RPE5.py ingest --help
python RPE5.py learn --help
python RPE5.py classify --help

# extract features
python RPE5.py ingest /corpus/family_1/ family:family_1,software:malware --concurrency 8
python RPE5.py ingest /corpus/family_2/ family:family_2,software:malware --concurrency 8

# if you only have extracted json from our repo and not the actual malware binaries:
python RPE5.py ingest path/to/json/data/features/ none --concurrency 8

# train models on extracted features
python RPE5.py learn

# classify unknown files
python RPE5.py classify /corpus/unknown/

# if you only have extracted json from our repo and not the actual malware binaries:
python RPE5.py classify path/to/json/data/features/

# testing (use subset of ingested files for training, use the balance for classification)
python RPE5.py learn --train-percent 70;python RPE5.py classify /corpus --train-percent 70

Design approach:
*  Filter out or disassociate fragile information from strong features.  For example, a section name is a strong feature, but its offset and size are likely to vary from one version of the binary to another.
*  Produce useful feature combinations (ordered pairs of metadata, structured string fragments)
*  Capture tool errors (which may indicate malformation) as features too.
*  Keep separate classes of features separate (in our case, strings, meta, disassembly).  This raises the bar for the obfuscator, as it must be effective in all feature classes in order to fully defeat detection.
*  Also consider combinations of feature classes (strings+meta, strings+disassembly, disassembly+meta).  There may be relevant relationships between disparate classes.
*  Leverage existing unpacker (in this case, ClamAV) and accumulate features from each extracted blob.  Each feature extractor has potential to save off any extracted blobs for re-ingestion.
*  Use more than one machine learning model type (in our case, random forest and nearest neighbor).  Combining different approaches has proven successful in other machine learning challenges (Netflix).
*  Use the separate feature classes, class combinations, and multiple machine learning models, to form a voting system.
*  Support arbitrary number of arbitrary label types.  Build up voting system for each type of label.
*  Use plugin model so additional feature types and classifiers can be added.
*  Design feature extraction plugins to also work standalone for rapid development and test cycle.
*  Built in train/test scoring model to monitor overall system accuracy.
*  Streamlined workflow (ingest, learn, classify).  Labels can be added/changed and re-learned.

If we had time...
*  JavaScript execution in sandbox using node.js VM2 inside a docker container with eval() replaced with console.log() and all long string variables extracted
*  Sandbox execution of major executable file types to capture system/API call and other behavioral information
*  Run all files through virus total.  Use results as features.
*  More testing, tuning, tweaking...  never enough...

"""

# Usability:
#   TODO review and comment all code
#   TODO --comment arg to ingest for adding comments to files
# Features extraction:
#   TODO reveiw feature extraction for files with excessive number of features, improve filtering as needed
#   TODO radare2 file type white list instead of black list
#   TODO string meta: line endings, encoding, indentation none/tab/space/mixed, commenting, keywords for js/html, base64
#   TODO string disassembly: js/html keywords
#   TODO disassembly ngrams, sort and take subset for order invariance?
#   TODO dissasembly capture symbol order
#   TODO improve support for jar, .net mono assembly, zip, javascript, html
#   TODO base64 and \xHH decoding
#   TODO imphash, pehash, richpe
#   TODO review authorship paper
# ML
#   TODO use predict_proba() to get confidence number
#   TODO change indentify.py into a classifier plugin.  filter out common and singleton features during learning?
# Test:
#   TODO (!) build up corpus with more file formats, benign file types to balance
#   TODO generate diversified files (UPX, replace strings, add junk or other file to end, blank parts of file)
#   TODO test, tune, tweak, hunt for edge cases
# Performance:
#   TODO multiprocessing more efficiently
# Future work:
#   TODO scale up
#   TODO GUI

import argparse
import collections
import csv
import hashlib
import importlib
import json
import os
import pickle
import shutil
import subprocess
import sys
import time
import traceback
import zlib

#################################################################
# Globals
#################################################################

KEEP_EXTRACTED_BLOBS = True

PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

EXTRACTOR_PLUGIN_DIR  = 'extractors'
IDENTIFIER_PLUGIN_DIR = 'identifiers'
CLASSIFIER_PLUGIN_DIR = 'classifiers'

DATABASE_PATH = os.path.join(PROJECT_ROOT, 'database')

#################################################################
# Code
#################################################################


class Storage(object):

    def __init__(self, root=DATABASE_PATH, read_only=False):
        self._root = root
        self._read_only = read_only
        if not os.path.exists(root):
            os.makedirs(root)

    @property
    def root(self):
        return self._root

    def store(self, subdirs, name, data, fmt='json'):
        if self._read_only:
            return
        base = os.path.join(self._root, subdirs)
        if not os.path.exists(base):
            os.makedirs(base)
        if fmt == 'json':
            fpath = os.path.join(base, name+'.json')
            with open(fpath, 'wb') as f:
                return f.write(json.dumps(data, indent=4).encode('utf-8', errors='replace'))
        if fmt == 'pickle':
            fpath = os.path.join(base, name+'.pickle')
            with open(fpath, 'wb') as f:
                return f.write(pickle.dumps(data))
        raise ValueError('Storage.store() format must be "json" or "pickle"')

    def load(self, subdirs, name):
        fpath = os.path.join(self._root, subdirs, name)
        if (not fpath.endswith('.json')) and (not fpath.endswith('.pickle')):
            if os.path.exists(fpath + '.json'):
                fpath += '.json'
            elif os.path.exists(fpath + '.pickle'):
                fpath += '.pickle'
        if fpath.endswith('.json'):
            try:
                with open(fpath, 'rb') as f:
                    return json.loads(f.read().decode('utf-8', errors='replace'))
            except (OSError, json.decoder.JSONDecodeError) as e:
                print('Error reading %s.  %s'%(fpath, e))
                return None
        if fpath.endswith('.pickle'):
            with open(fpath, 'rb') as f:
                return pickle.loads(f.read())
        raise FileNotFoundError('Unable to find ' + fpath)

    def load_all(self, subdirs):
        base = os.path.join(self._root, subdirs)
        if os.path.exists(base):
            for name in os.listdir(base):
                if name.endswith('.json') or name.endswith('.pickle'):
                    yield self.load(subdirs, name)

    def self_test(self):
        data1 = [1, 2, 3]
        data2 = [4, 5, 6]
        data3 = [7, 8, 9]
        self.store('test/a', 'file1', data1)
        self.store('test/a', 'file2', data2)
        self.store('test/a', 'file3', data3, 'pickle')
        assert self.load('test/a', 'file1') == data1
        assert self.load('test/a', 'file2') == data2
        assert self.load('test/a', 'file3') == data3
        result = list(self.load_all('test/a'))
        assert data1 in result
        assert data2 in result
        assert data3 in result


def md5sum(filepath):
    """return md5 sum of file"""
    hasher = hashlib.md5()
    with open(filepath, 'rb') as f:
        hasher.update(f.read())
    return hasher.hexdigest()


def filetype(filepath):
    """run host's "file" command to get type"""
    output = subprocess.check_output(['file', filepath])
    output = output.decode('utf-8').split(':', 1)[-1].strip()  # remove file name
    return output


def labels2str(labels_dict):
    """
    convert the labels dictionary into a string.
    in:    {'key1':value1, 'key2':value2}
    out:   "key1:value1,key2:value2"
    """
    out = []
    for k,v in labels_dict.items():
        out.append(k+':'+v)
    out.sort()
    return ','.join(out)


def str2labels(labels_str):
    """
    convert a label string into a dictionary.
    in:    "key1:value1,key2:value2"
    out:   {'key1':value1, 'key2':value2}
    """
    labels_dict = {}
    if labels_str and labels_str.lower() != 'none':
        for entry in labels_str.split(','):
            k,v = entry.split(':')
            k = k.strip()
            v = v.strip()
            labels_dict[k.lower().strip()] = v
    return labels_dict


def init_fileinfo(fileinfo=None, filepath=''):
    """initialize fileinfo dict"""
    if fileinfo is None:
        fileinfo_out = {
            'md5': None,
            'type': None,
            'clamav': None,
        }
    else:
        # make a copy
        fileinfo_out = {}
        for k,v in fileinfo.items():
            fileinfo_out[k] = v
    if filepath:
        if ('md5' not in fileinfo_out) or (not fileinfo_out['md5']):
            fileinfo_out['md5'] = md5sum(filepath)
        if ('type' not in fileinfo_out) or (not fileinfo_out['type']):
            fileinfo_out['type'] = filetype(filepath)
    if 'labels' not in fileinfo_out:
        fileinfo_out['labels'] = {}
    return fileinfo_out


def load_plugins(plugin_dir):
    """return list of plugin modules imported from plugin_dir located in PROJECT_ROOT"""
    plugins = []

    plugin_path = os.path.join(PROJECT_ROOT, plugin_dir)
    # print('loading plugins from '+plugin_path)

    for plugin_py in os.listdir(plugin_path):
        # print(plugin_py)
        if plugin_py.startswith('_'):
            continue
        if not plugin_py.endswith('.py'):
            continue
        plugin_name = plugin_py[0:-3]
        plugin_mod = importlib.import_module('.'.join([plugin_dir, plugin_name]))
        if plugin_mod.__enabled__:
            plugins.append(plugin_mod)

    # print(plugins)
    return plugins


def load_all_plugins():

    all_plugins = {
        'ex_plugins': load_plugins(EXTRACTOR_PLUGIN_DIR),
        'id_plugins': load_plugins(IDENTIFIER_PLUGIN_DIR),
        'class_plugins': load_plugins(CLASSIFIER_PLUGIN_DIR),
    }
    return all_plugins


def run_plugins(plugins, filepath, fileinfo, **kwargs):

    results = {
        'filepath': filepath,
        'fileinfo': fileinfo,
        'plugins': [],
    }
    for plugin in plugins:
        result = None
        try:
            result = plugin.run(filepath, fileinfo, **kwargs)
        except KeyboardInterrupt:
            raise
        except Exception:
            print(traceback.format_exc())
        results['plugins'].append({
            'plugin': plugin.__name__,
            'version': plugin.__version__,
            'results': result,
        })
        # print('Plugin %s had %d results.' % (plugin.__name__, len(result)))
    return results


def get_a_filepath(rootpath, skip_set):
    """
    walk directory and return first file found that isn't in skip_set
    """
    for root, dirs, files in os.walk(rootpath):
        for file in files:
            filepath = os.path.join(root, file)
            if filepath not in skip_set:
                return filepath
    return None


def extract_features_loop(filepath, fileinfo, storage, ex_plugins):
    """
    Run plugins to extract features.
    plugins may also unpack blobs into temp directory,
    so loop over any blobs and re-extract features
    """

    # place to save any unpacked blobs
    temppath = os.path.join(storage.root, 'temp', fileinfo['md5'])
    if not os.path.exists(temppath):
        os.makedirs(temppath)

    # first pass extracting features
    results = run_plugins(ex_plugins, filepath, fileinfo, temppath=temppath)

    # loop over any blobs
    blob_count = 0
    processed_file_paths = set()
    processed_file_hashes = set()
    blob_stats = collections.Counter()
    short_file_types = collections.Counter()
    while True:

        # are there any blobs we haven't processed?
        blob_path = get_a_filepath(temppath, processed_file_paths)
        if not blob_path:
            break
        processed_file_paths.add(blob_path)

        # file info
        blob_fileinfo = init_fileinfo({}, blob_path)
        if blob_fileinfo['md5'] in processed_file_hashes:
            continue
        processed_file_hashes.add(blob_fileinfo['md5'])
        blob_size = os.path.getsize(blob_path)
        print('    Extracted blob, size %d, type %s' % (blob_size, blob_fileinfo['type']))

        # stats
        blob_count += 1
        blob_stats['Extracted blob: '+blob_fileinfo['type']] += 1
        short_file_types[blob_fileinfo['type'].split(' ', 1)[0]] += 1

        # get features from blob
        if not blob_fileinfo['type'].startswith('PNG'):
            # note: we are ignoring PNG files because there are a lot of icons
            # TODO better feature extraction for images
            more_results = run_plugins(ex_plugins, blob_path, blob_fileinfo, temppath=temppath)

        # # any updated clamav results?
        # if (blob_fileinfo['clamav'] is not None) and ('OK' not in blob_fileinfo['clamav']):
        #     # clam classified the blob as bad
        #     if (fileinfo['clamav'] is None) or ('OK' in blob_fileinfo['clamav']):
        #         # original file was not recognized by clam as bad
        #         # apply blob classification to parent file
        #         fileinfo['clamav'] = blob_fileinfo['clamav']#

        # merge into results
        for plugin1 in results['plugins']:
            for plugin2 in more_results['plugins']:
                if plugin1['plugin'] == plugin2['plugin']:
                    plugin1['results'] += plugin2['results']

        # remove blob, we are done with it
        if not KEEP_EXTRACTED_BLOBS:
            os.remove(blob_path)

    # summarise blob stats
    blob_summary = None
    if blob_count > 0:
        blob_summary = 'Extracted %d blobs (%s)' % (blob_count, ', '.join(['%d %s' % (v, k) for k,v in short_file_types.items()]))
        print('    ' + blob_summary)
    results['fileinfo']['blobs'] = blob_summary
    sft = list(short_file_types.keys())
    sft.sort()
    blob_stats['blob file types: '+', '.join(sft)] += 1

    # add blob stats to meta
    for plugin1 in results['plugins']:
        if 'meta' in plugin1['plugin']:
            plugin1['results'] += blob_stats

    # clean up
    if not KEEP_EXTRACTED_BLOBS:
        shutil.rmtree(temppath)
    return results


def extract_features(filepath, fileinfo, storage, ex_plugins, fresh=True):
    print('')
    print(filepath)
    results = None

    if filepath.endswith('.json'):
        # this is a feature extraction file and not a malware sample
        # reuse provided results
        with open(filepath, 'rb') as f:
            results = json.loads(f.read().decode('utf-8', errors='replace'))
        # sanity check file format
        for k in ['filepath', 'fileinfo', 'plugins']:
            if k not in results:
                raise ValueError('Invalid json feature file, expecting "%s"' % k)
        print('    Original file: '+results['filepath'])
        # merge labels
        for k,v in fileinfo['labels'].items():
            results['fileinfo']['labels'][k] = v
        fileinfo = results['fileinfo']
        storage.store('features', fileinfo['md5'], results)

    elif not fresh:
        # reuse existing results if available
        try:
            results = storage.load('features', fileinfo['md5'])
            print('    Reusing existing feature extraction')
        except FileNotFoundError:
            pass

    if results is None:
        # ok, we actually need to extract features
        results = extract_features_loop(filepath, fileinfo, storage, ex_plugins)
        storage.store('features', fileinfo['md5'], results)
    print('    Info:')
    print('        MD5: '+str(results['fileinfo'].get('md5', 'unknown')))
    print('        Type: '+str(results['fileinfo'].get('type', 'unknown')))
    print('        ClamAV: '+str(results['fileinfo'].get('clamav', 'unknown')))
    print('        Labels: '+str(labels2str(results['fileinfo']['labels'])))
    return results


def identify_file(filepath, fileinfo, feature_data, storage, training, id_plugins, fresh=True):

    if not fresh:
        # return existing results if available
        try:
            results = storage.load('identities/minhashes', fileinfo['md5'])
            print('Reusing existing identification')
            return results
        except FileNotFoundError:
            pass
    results = run_plugins(id_plugins, filepath, fileinfo, feature_data=feature_data, storage=storage, training=training)
    storage.store('identities/minhashes', fileinfo['md5'], results)
    return results


def ingest_file(filepath, fileinfo, storage, training, all_plugins, fresh):
    fileinfo = init_fileinfo(fileinfo, filepath)
    feature_data = extract_features(filepath, fileinfo, storage, all_plugins['ex_plugins'], fresh)
    identity_data = identify_file(filepath, fileinfo, feature_data, storage, training, all_plugins['id_plugins'], fresh)
    return feature_data, identity_data


def ingest(args, storage=None, all_plugins=None):

    # multi-processing
    concurrency = args.concurrency
    thread_id = args.thread_id
    if concurrency > 1 and thread_id is None:
        # spin up subprocess
        processes = []
        for tid in range(concurrency):
            cmd = [sys.executable] + sys.argv + ['--thread-id', str(tid)]
            print(' '.join(cmd))
            processes.append(subprocess.Popen(cmd))
        for proc in processes:
            proc.wait()
        return

    if storage is None:
        storage = Storage(args.database)
    if all_plugins is None:
        all_plugins = load_all_plugins()
    fresh = args.fresh

    fileinfo = {
        'labels': str2labels(args.labels),
    }
    training = True

    if os.path.isfile(args.filepath):
        ingest_file(args.filepath, fileinfo, storage, training, all_plugins, args.fresh)

    elif os.path.isdir(args.filepath):
        for root, dirs, files in os.walk(args.filepath):
            for file in files:
                if file == '.DS_Store':
                    continue  # skip Mac OS file system data

                # for multiprocessing, only process files corresponding with thread_id
                if concurrency > 1:
                    if zlib.crc32(file.encode('utf-8')) % concurrency != thread_id:
                        continue

                filepath = os.path.join(root, file)
                ingest_file(filepath, fileinfo, storage, training, all_plugins, fresh)
    else:
        raise FileNotFoundError('can\'t find '+args.filepath)


def learn(args, storage=None):

    if storage is None:
        storage = Storage(args.database)
    class_plugins = load_plugins(CLASSIFIER_PLUGIN_DIR)

    train_percent = args.train_percent
    rand_seed = args.seed

    results = run_plugins(class_plugins, filepath=None, fileinfo=None, storage=storage, mode='learn',
                          train_percent=train_percent, rand_seed=rand_seed)
    return results


def classify_file(filepath, fileinfo=None, storage=None, all_plugins=None,
                      train_percent=100, rand_seed='0', fresh=False):

    if fileinfo is None:
        fileinfo = init_fileinfo(filepath=filepath)
    if storage is None:
        storage = Storage()
    if all_plugins is None:
        all_plugins = load_all_plugins()

    training = False

    feature_data = extract_features(filepath, fileinfo, storage, all_plugins['ex_plugins'], fresh)

    fileinfo = feature_data['fileinfo']  # fileinfo may get enriched by extraction

    classify_data = run_plugins(all_plugins['class_plugins'], filepath=filepath, fileinfo=fileinfo, storage=storage,
                                mode='classify', file_data=feature_data, train_percent=train_percent, rand_seed=rand_seed)
    identity_data = identify_file(filepath, fileinfo, feature_data, storage, training, all_plugins['id_plugins'])
    return classify_data, identity_data


def classify(args, storage=None, all_plugins=None):
    if storage is None:
        storage = Storage(args.database, read_only=False)
    if all_plugins is None:
        all_plugins = load_all_plugins()

    train_percent = args.train_percent
    rand_seed = args.seed
    fresh = args.fresh

    if os.path.isfile(args.filepath):
        classify_file(args.filepath, fileinfo=None, storage=storage, all_plugins=all_plugins,
                      train_percent=train_percent, rand_seed=rand_seed, fresh=fresh)
    elif os.path.isdir(args.filepath):
        class_counter = collections.Counter()
        success_count = 0
        total_count = 0
        error_list = []
        try:
            for root, dirs, files in os.walk(args.filepath):
                for file in files:
                    if file == '.DS_Store':
                        continue  # skip Mac OS file system data
                    filepath = os.path.join(root, file)
                    classify_data,identify_data = classify_file(filepath, fileinfo=None, storage=storage,
                        all_plugins=all_plugins, train_percent=train_percent, rand_seed=rand_seed, fresh=fresh)

                    # compare prediction against any labels we have
                    fileinfo = classify_data['fileinfo']
                    for classify_plugin in classify_data['plugins']:
                        plugin_name = classify_plugin['plugin']
                        for classified_as in classify_plugin['results'].split(','):
                            label_k = classified_as.split(':')[0]
                            label_v = classified_as.split(':')[-1]
                            if 'labels' in fileinfo and label_k in fileinfo['labels']:
                                actual_v = fileinfo['labels'][label_k]
                                if actual_v == label_v:
                                    success_count += 1
                                else:
                                    error_list.append(fileinfo['md5']+' is '+actual_v+' but classified as '+label_v)
                                total_count += 1
                            else:
                                actual_v = '?'
                            actual_classification = label_k+':'+actual_v
                            result_str = plugin_name + ' classified ' + actual_classification + ' as ' + classified_as
                            class_counter[result_str] += 1

        except KeyboardInterrupt:
            print('')
            print('Exit early due to KeyboardInterrupt')
            print('')

        for result_str in class_counter:
            print(result_str + ': '+str(class_counter[result_str]))
        for err in error_list:
            print(err)
        if total_count > 0:
            print('Accuracy: '+str(100*success_count/total_count))
    else:
        raise FileNotFoundError('can\'t find '+args.filepath)


def count_results(filedata, plugin_name):
    for plugin in filedata['plugins']:
        if plugin['plugin'] == plugin_name:
            return len(plugin['results'])
    return None


def export_csv(args):
    storage = Storage(read_only=True)
    plugin_names = [p.__name__ for p in load_plugins(EXTRACTOR_PLUGIN_DIR)]
    with open(args.output, mode='wt') as out_file:
        writer = csv.writer(out_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)

        writer.writerow([
            'MD5',
            'Filepath',
            'Labels',
            'Type',
            'ClamAV',
            'Blobs',
        ]+plugin_names)

        for filedata in storage.load_all('features'):
            writer.writerow([
                filedata['fileinfo']['md5'],
                filedata['filepath'],
                labels2str(filedata['fileinfo']['labels']),
                filedata['fileinfo']['type'],
                filedata['fileinfo']['clamav'],
                filedata['fileinfo']['blobs'],
            ]+[count_results(filedata,p) for p in plugin_names])


def import_csv(args):
    storage = Storage()
    with open(args.input, mode='rt') as in_file:
        reader = csv.reader(in_file, delimiter=',')
        index = {}
        line_num = 0
        for row in reader:
            line_num += 1
            if line_num == 1:
                for i in range(len(row)):
                    index[row[i]] = i
            else:
                md5 = row[index['MD5']]
                new_labels = row[index['Labels']]

                filedata = storage.load('features', md5)
                existing_labels = labels2str(filedata['fileinfo']['labels'])

                if existing_labels != new_labels:
                    print('label for %s changed from "%s" to "%s"' % (md5, existing_labels, new_labels))
                    filedata['fileinfo']['labels'] = str2labels(new_labels)
                    storage.store('features', md5, filedata)


def get_arg_parser():
    parser = argparse.ArgumentParser(description='RPE5 malware feature extractor and classifier')
    parser.add_argument('--database', type=str, help='path for storing all ingested data and derived models',
                        default=DATABASE_PATH)
    subparsers = parser.add_subparsers(help='Commands', dest='command')

    # "ingest" command
    parser_ingest = subparsers.add_parser('ingest', help='Extract features from files')
    parser_ingest.add_argument('filepath', type=str, help='Path to file or directory of files')
    parser_ingest.add_argument('labels', type=str,
                               help='"none" or dictionary of labels, for example "family:family_1,author:fred".  '
                               "You can use any labels you want.",)
    parser_ingest.add_argument('--fresh', action='store_true',
                               help='perform fresh extraction on files that have been previously extracted')
    thread_group = parser_ingest.add_argument_group('Threading')
    thread_group.add_argument('--concurrency', type=int, help='Number of concurrent processes to run', default=1)
    thread_group.add_argument('--thread-id', type=int, help='Use --concurrency instead', default=None)

    # "learn" command
    parser_learn = subparsers.add_parser('learn', help='Build ML models based on previously ingested files')
    test_group = parser_learn.add_argument_group('Testing')
    test_group.add_argument('--train-percent', type=float, help='Percentage of files to learn from', default=100)
    test_group.add_argument('--seed', type=str, help='Random seed for file selection', default='0')

    # "classify" command
    parser_classify = subparsers.add_parser('classify', help='classify files')
    parser_classify.add_argument('filepath', type=str, help='Path to file or directory of files')
    parser_classify.add_argument('--fresh', action='store_true',
                               help='perform fresh extraction on files that have been previously extracted')
    test_group = parser_classify.add_argument_group('Testing')
    test_group.add_argument('--train-percent', type=float, default=100,
                            help='Percentage of files used in learn stage.  Inverse will be used for test.')
    test_group.add_argument('--seed', type=str, help='Random seed for file selection', default='0')

    # "export" command
    parser_export = subparsers.add_parser('export', help='export file info to csv')
    parser_export.add_argument('-o', '--output', default='fileinfo.csv', help='Output file')

    # "import" command
    parser_import = subparsers.add_parser('import', help='import file info from csv')
    parser_import.add_argument('-i', '--input', default='fileinfo.csv', help='Input file')

    return parser


def main(argv):
    start_time = time.time()
    parser = get_arg_parser()
    args = parser.parse_args(argv)
    try:
        if args.command == 'ingest':
            ingest(args)
        elif args.command == 'learn':
            learn(args)
        elif args.command == 'classify':
            classify(args)
        elif args.command == 'export':
            export_csv(args)
        elif args.command == 'import':
            import_csv(args)
        else:
            print('Unknown command')
            parser.print_usage()
            exit(-1)
    except Exception as e:
        raise e
    finally:
        print("run time: %0.1f seconds" % (time.time() - start_time))

    try:
        thread_id = args.thread_id
    except AttributeError:
        thread_id = None
    if thread_id is not None:
        print("done thread %d." % args.thread_id)
    else:
        print("done.")


if __name__ == '__main__':
    main(sys.argv[1:])


