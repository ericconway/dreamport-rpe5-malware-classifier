"""
Usage:

# help
python RPE5.py --help
python RPE5.py ingest --help
python RPE5.py learn --help
python RPE5.py classify --help

# extract features
python RPE5.py ingest /corpus/malware/ malware
python RPE5.py ingest /corpus/benign/ benign

# train models on extracted features
python RPE5.py learn

# classify files
python RPE5.py classify /corpus/unknown/

# testing (use subset of ingested files for training, use the balance for classification)
python RPE5.py learn --train-percent 70;python RPE5.py classify /corpus --train-percent 70

"""

# Usability:
#   TODO install script for pip + apt requirements
#   TODO review and comment all code
#   TODO export-info, import-info,
#   TODO html report generation
#   TODO train-test use labels instead of file paths?
#   TODO option to import json and skip feature extraction
#   TODO ability to unzip files
# Features extraction:
#   TODO capture file type
#   TODO sort disassembly, throw out nops, throw out more
#   TODO jar
#   TODO .net mono assembly
#   TODO zip
#   TODO javascript
#   TODO html
#   TODO combinations of features (strings+disassembly, ...)
#   TODO filter out common features
#   TODO clamav debug scan
#   TODO clamav save temp unpack
#   TODO stderr from radare as feature
#   TODO unicode strings?
#   TODO tune thresholds
#   TODO imphash, pehash, richpe
#   TODO review authorship paper
# Other:
#   TODO classify based on authorship / campaign
#   TODO performance counter
#   TODO cache file hashes?
#   TODO consider separate db per file type?
# Test dev:
#   TODO build up corpus with more file formats
#   TODO test on various file formats
#   TODO scan corpus for file format bias
#   TODO generate diversified files
# Test
#   TODO number of features passed to sklearn
#   TODO feature counter vs list
# Performance:
#   TODO multiprocessing / concurrency

####################################
# Questions for organizers
#    What file types will be included in the sample corpus?  (PE, ELF, MachO, Python, ...)
#    Roughly how many file should we expect in the competition corpus?
#    Will the corpus include files labeled with author and campaign?  If so, how many files should we expect to be labeled?
#    Are PCAP files part of this RPE?  If so, how will they be used?
#    How many sample files will each offensive team generate?  Of those, how many will be labeled for training and how many will be unlabeled for testing?
#    Do the offensive teams have access to malware source code, or only compiled malware binaries?
#    Can the defensive teams use public internet connected services?  Can they use private internet connected resources?
#    Can defensive teams incorporation commercial and/or open source antivirus products?


import argparse
import collections
import hashlib
import importlib
import json
import os
import pickle
import sys
import time
import traceback
import zlib

#################################################################
# Globals
#################################################################

PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

EXTRACTOR_PLUGIN_DIR  = 'extractors'
IDENTIFIER_PLUGIN_DIR = 'identifiers'
CLASSIFIER_PLUGIN_DIR = 'classifiers'

DATABASE_PATH = os.path.join(PROJECT_ROOT, 'database')

#################################################################
# Code
#################################################################


class Storage(object):

    def __init__(self, root=DATABASE_PATH, read_only=False):
        self._root = root
        self._read_only = read_only
        if not os.path.exists(root):
            os.makedirs(root)

    def store(self, subdirs, name, data, fmt='json'):
        if self._read_only:
            return
        base = os.path.join(self._root, subdirs)
        if not os.path.exists(base):
            os.makedirs(base)
        if fmt == 'json':
            fpath = os.path.join(base, name+'.json')
            with open(fpath, 'wb') as f:
                return f.write(json.dumps(data, indent=4).encode('utf-8', errors='replace'))
        if fmt == 'pickle':
            fpath = os.path.join(base, name+'.pickle')
            with open(fpath, 'wb') as f:
                return f.write(pickle.dumps(data))
        raise ValueError('Storage.store() format must be "json" or "pickle"')

    def load(self, subdirs, name):
        fpath = os.path.join(self._root, subdirs, name)
        if (not fpath.endswith('.json')) and (not fpath.endswith('.pickle')):
            if os.path.exists(fpath + '.json'):
                fpath += '.json'
            elif os.path.exists(fpath + '.pickle'):
                fpath += '.pickle'
        if fpath.endswith('.json'):
            try:
                with open(fpath, 'rb') as f:
                    return json.loads(f.read().decode('utf-8', errors='replace'))
            except (OSError, json.decoder.JSONDecodeError) as e:
                print('Error reading %s.  %s'%(fpath, e))
                return None
        if fpath.endswith('.pickle'):
            with open(fpath, 'rb') as f:
                return pickle.loads(f.read())
        raise FileNotFoundError('Unable to find ' + fpath)

    def load_all(self, subdirs):
        base = os.path.join(self._root, subdirs)
        if os.path.exists(base):
            for name in os.listdir(base):
                if name.endswith('.json') or name.endswith('.pickle'):
                    yield self.load(subdirs, name)

    def self_test(self):
        data1 = [1, 2, 3]
        data2 = [4, 5, 6]
        data3 = [7, 8, 9]
        self.store('test/a', 'file1', data1)
        self.store('test/a', 'file2', data2)
        self.store('test/a', 'file3', data3, 'pickle')
        assert self.load('test/a', 'file1') == data1
        assert self.load('test/a', 'file2') == data2
        assert self.load('test/a', 'file3') == data3
        result = list(self.load_all('test/a'))
        assert data1 in result
        assert data2 in result
        assert data3 in result


def md5sum(filepath):
    """return md5 sum of file"""
    hasher = hashlib.md5()
    with open(filepath, 'rb') as f:
        hasher.update(f.read())
    return hasher.hexdigest()


def load_plugins(plugin_dir):
    """return list of plugin modules imported from plugin_dir located in PROJECT_ROOT"""
    plugins = []

    plugin_path = os.path.join(PROJECT_ROOT, plugin_dir)
    # print('loading plugins from '+plugin_path)

    for plugin_py in os.listdir(plugin_path):
        # print(plugin_py)
        if plugin_py.startswith('_'):
            continue
        if not plugin_py.endswith('.py'):
            continue
        plugin_name = plugin_py[0:-3]
        plugin_mod = importlib.import_module('.'.join([plugin_dir, plugin_name]))
        if plugin_mod.__enabled__:
            plugins.append(plugin_mod)

    # print(plugins)
    return plugins


def load_all_plugins():

    all_plugins = {
        'ex_plugins': load_plugins(EXTRACTOR_PLUGIN_DIR),
        'id_plugins': load_plugins(IDENTIFIER_PLUGIN_DIR),
        'class_plugins': load_plugins(CLASSIFIER_PLUGIN_DIR),
    }
    return all_plugins


def run_plugins(plugins, filepath, fileinfo, **kwargs):

    results = {
        'filepath': filepath,
        'fileinfo': fileinfo,
        'plugins': [],
    }
    for plugin in plugins:
        result = None
        try:
            result = plugin.run(filepath, fileinfo, **kwargs)
        except KeyboardInterrupt:
            raise
        except Exception:
            print(traceback.format_exc())
        results['plugins'].append({
            'plugin': plugin.__name__,
            'version': plugin.__version__,
            'results': result,
        })
        # print('Plugin %s had %d results.' % (plugin.__name__, len(result)))
    return results


def extract_features(filepath, fileinfo, storage, ex_plugins, fresh=True):

    if not fresh:
        # return existing results if available
        try:
            results = storage.load('features', fileinfo['md5'])
            print('Reusing existing feature extraction')
            return results
        except FileNotFoundError:
            pass
    results = run_plugins(ex_plugins, filepath, fileinfo)
    storage.store('features', fileinfo['md5'], results)
    return results


def identify_file(filepath, fileinfo, feature_data, storage, training, id_plugins, fresh=True):

    if not fresh:
        # return existing results if available
        try:
            results = storage.load('identities/minhashes', fileinfo['md5'])
            print('Reusing existing identification')
            return results
        except FileNotFoundError:
            pass
    results = run_plugins(id_plugins, filepath, fileinfo, feature_data=feature_data, storage=storage, training=training)
    storage.store('identities/minhashes', fileinfo['md5'], results)
    return results


def ingest_file(filepath, fileinfo, storage, training, all_plugins, fresh):

    fileinfo['md5'] = md5sum(filepath)
    feature_data = extract_features(filepath, fileinfo, storage, all_plugins['ex_plugins'], fresh)
    identity_data = identify_file(filepath, fileinfo, feature_data, storage, training, all_plugins['id_plugins'], fresh)
    return feature_data, identity_data


def ingest(args, storage=None, all_plugins=None):

    if storage is None:
        storage = Storage(args.database)
    if all_plugins is None:
        all_plugins = load_all_plugins()

    concurrency = args.concurrency
    thread_id = args.thread_id
    thread_count = args.thread_count
    fresh = args.fresh

    fileinfo = {
        'classification': args.classification,
        'author': args.author,
        'campaign': args.campaign,
    }
    training = True

    if os.path.isfile(args.filepath):
        ingest_file(args.filepath, fileinfo, storage, training, all_plugins)

    elif os.path.isdir(args.filepath):
        for root, dirs, files in os.walk(args.filepath):
            for file in files:
                if file == '.DS_Store':
                    continue  # skip Mac OS file system data

                # for multiprocessing, only process files corresponding with thread_id
                if thread_count > 1:
                    if zlib.crc32(file.encode('utf-8')) % thread_count != thread_id:
                        continue

                filepath = os.path.join(root, file)
                ingest_file(filepath, fileinfo, storage, training, all_plugins, fresh)
    else:
        raise FileNotFoundError('can\'t find '+args.filepath)


def learn(args, storage=None):

    if storage is None:
        storage = Storage(args.database)
    class_plugins = load_plugins(CLASSIFIER_PLUGIN_DIR)

    train_percent = args.train_percent
    rand_seed = args.seed

    results = run_plugins(class_plugins, filepath=None, fileinfo=None, storage=storage, mode='learn',
                          train_percent=train_percent, rand_seed=rand_seed)
    return results


def classify_file(filepath, fileinfo=None, storage=None, all_plugins=None,
                      train_percent=100, rand_seed='0', fresh=True):

    if fileinfo is None:
        fileinfo = {}
    if 'md5' not in fileinfo:
        fileinfo['md5'] = md5sum(filepath)

    if storage is None:
        storage = Storage()
    if all_plugins is None:
        all_plugins = load_all_plugins()

    training = False

    feature_data = extract_features(filepath, fileinfo, storage, all_plugins['ex_plugins'], fresh)

    classify_data = run_plugins(all_plugins['class_plugins'], filepath=None, fileinfo=None, storage=storage,
                                mode='classify', file_data=feature_data, train_percent=train_percent, rand_seed=rand_seed)
    identity_data = identify_file(filepath, fileinfo, feature_data, storage, training, all_plugins['id_plugins'])
    return classify_data, identity_data


def classify(args, storage=None, all_plugins=None):
    if storage is None:
        storage = Storage(args.database, read_only=True)
    if all_plugins is None:
        all_plugins = load_all_plugins()

    train_percent = args.train_percent
    rand_seed = args.seed
    fresh = args.fresh

    if os.path.isfile(args.filepath):
        classify_file(args.filepath, fileinfo=None, storage=storage, all_plugins=all_plugins,
                      train_percent=train_percent, rand_seed=rand_seed, fresh=fresh)
    elif os.path.isdir(args.filepath):
        class_counter = collections.Counter()
        success_count = 0
        try:
            for root, dirs, files in os.walk(args.filepath):
                for file in files:
                    if file == '.DS_Store':
                        continue  # skip Mac OS file system data
                    filepath = os.path.join(root, file)
                    results = classify_file(filepath, fileinfo=None, storage=storage, all_plugins=all_plugins,
                                            train_percent=train_percent, rand_seed=rand_seed, fresh=fresh)

                    # test how well we are doing:
                    actual_classification = 'unknown'
                    if 'malware' in filepath.lower():
                        actual_classification = 'malware'
                    elif 'benign' in filepath.lower():
                        actual_classification = 'benign'
                    for plugin in results[0]['plugins']:
                        plugin_name = plugin['plugin']
                        classified_as = plugin['results']
                        if classified_as is not None:
                            if actual_classification == classified_as:
                                success_count += 1
                            result_str = plugin_name + ' classified '+actual_classification+' as '+classified_as
                            class_counter[result_str] += 1
        except KeyboardInterrupt:
            print('Exit early due to KeyboardInterrupt')
        for result_str in class_counter:
            print(result_str + ': '+str(class_counter[result_str]))
        if sum(class_counter.values()) > 0:
            print('Accuracy: '+str(100*success_count/sum(class_counter.values())))
    else:
        raise FileNotFoundError('can\'t find '+args.filepath)


def parse_args(argv):
    parser = argparse.ArgumentParser(description='RPE5 malware feature extractor and classifier')
    parser.add_argument('--database', type=str, help='path for storing all ingested data and derived models',
                        default=DATABASE_PATH)
    subparsers = parser.add_subparsers(help='Commands', dest='command')

    # "ingest" command
    parser_ingest = subparsers.add_parser('ingest', help='Extract features from files')
    parser_ingest.add_argument('filepath', type=str, help='Path to file or directory of files')
    parser_ingest.add_argument('classification', type=str, help='Classification (malware or benign)',
                               choices=['malware', 'benign'])
    parser_ingest.add_argument('--fresh', action='store_true',
                               help='perform fresh extraction on files that have been previously extracted')
    label_group = parser_ingest.add_argument_group('Extra Labels')
    label_group.add_argument('--author', type=str, help='Author', default=None)
    label_group.add_argument('--campaign', type=str, help='Campaign', default=None)
    thread_group = parser_ingest.add_argument_group('Threading')
    thread_group.add_argument('--concurrency', type=int, help='Number of concurrent processes to run', default=1)
    thread_group.add_argument('--thread-count', type=int, help='Use --concurrency instead', default=1)
    thread_group.add_argument('--thread-id', type=int, help='Use --concurrency instead', default=0)

    # "learn" command
    parser_learn = subparsers.add_parser('learn', help='Build ML models based on previously ingested files')
    test_group = parser_learn.add_argument_group('Testing')
    test_group.add_argument('--train-percent', type=float, help='Percentage of files to learn from', default=100)
    test_group.add_argument('--seed', type=str, help='Random seed for file selection', default='0')

    # "classify" command
    parser_classify = subparsers.add_parser('classify', help='classify files')
    parser_classify.add_argument('filepath', type=str, help='Path to file or directory of files')
    parser_classify.add_argument('--fresh', action='store_true',
                               help='perform fresh extraction on files that have been previously extracted')
    test_group = parser_classify.add_argument_group('Testing')
    test_group.add_argument('--train-percent', type=float, default=100,
                            help='Percentage of files used in learn stage.  Inverse will be used for test.')
    test_group.add_argument('--seed', type=str, help='Random seed for file selection', default='0')

    args = parser.parse_args(argv)
    return args


def main(argv):
    start_time = time.time()
    args = parse_args(argv)
    try:
        if args.command == 'ingest':
            ingest(args)
        elif args.command == 'learn':
            learn(args)
        elif args.command == 'classify':
            classify(args)
        else:
            raise Exception('Unknown command')
    except Exception as e:
        raise e
    finally:
        print("run time: %0.1f seconds" % (time.time() - start_time))
    print("done.")


if __name__ == '__main__':
    main(sys.argv[1:])


