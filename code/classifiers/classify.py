from collections import Counter, defaultdict
import itertools
import random
from sklearn.feature_extraction import FeatureHasher
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF


__version__ = '0.1'
__enabled__ = True

N_FEATURES = 2**19  # Bigger numbers give slightly better classifications, but are slower
FEATURE_TYPE = 'string'
g_classifiers = None

CLASSIFIER_CLASSES_KWARGS = [
    (RandomForestClassifier, {}),
    (KNeighborsClassifier, {'n_neighbors': 3}),
    # (SVC, {'gamma': 2, 'C': 1}),
    # (GaussianProcessClassifier, {'kernel': 1.0 * RBF(1.0)}),
]


def run(filepath='', fileinfo=None, storage=None, file_data=None, mode='learn', train_percent=100, rand_seed='0', **kwargs):
    # print("%s.run()" % __name__)

    # this will convert our features into a uniform format for sklearn
    hasher = FeatureHasher(n_features=N_FEATURES, input_type=FEATURE_TYPE)

    if mode == 'learn':
        return learn(storage, hasher, train_percent, rand_seed)
    else:
        return classify(storage, hasher, file_data, train_percent, rand_seed)


def file_is_for_training(md5, train_percent, rand_seed):
    random.seed(md5+rand_seed)
    return 100.0 * random.random() < train_percent


def learn(storage, hasher, train_percent, rand_seed):

    # load all feature data we have collected
    print('loading all data')
    all_features = storage.load_all('features')

    # next we will reformat feature data and associated labels into
    # a dict named feature_db organized by [label][plugin] as follows:
    # {
    #     'family':
    #     {
    #         'strings':
    #         {
    #             'labels': ['family_1', 'family_2', 'family_1', ... ],
    #             'features': [<hashed_features>, <hashed_features>, <hashed_features>, ...],
    #         },
    #         'meta':
    #         {
    #             'labels': ['Fred', 'Wilma', 'Barney', ... ],
    #             'features': [<hashed_features>, <hashed_features>, <hashed_features>, ...],
    #         },
    #         'disassembly':
    #         {
    #             'labels': ['Fred', 'Wilma', 'Barney', ... ],
    #             'features': [<hashed_features>, <hashed_features>, <hashed_features>, ...],
    #         },
    #     },
    #     'author':
    #     {
    #         'strings':
    #         {
    #             ...
    #         },
    #         ...
    #     },
    #     ...
    # }
    feature_db = defaultdict(lambda: defaultdict(lambda: {'labels': [], 'features': []}))
    for file_data in all_features:

        if not file_data:
            continue

        if train_percent < 100:
            # only use some files
            if not file_is_for_training(file_data['fileinfo']['md5'], train_percent, rand_seed):
                # skip here, use this file in test instead
                continue

        # only consider files that are labeled in some way
        if 'labels' in file_data['fileinfo']:
            if file_data['fileinfo']['labels'] is not None:

                for label_k, label_v in file_data['fileinfo']['labels'].items():

                    # loop over plugins
                    for plugin_data in get_plugin_data(file_data):

                        # insert labels and hashed features
                        plugin_name = plugin_data['plugin']
                        feature_db[label_k][plugin_name]['labels'].append(label_v)
                        if FEATURE_TYPE == 'string':
                            feature_db[label_k][plugin_name]['features'].append(plugin_data['results'].keys())
                        elif FEATURE_TYPE == 'dict':
                            feature_db[label_k][plugin_name]['features'].append(plugin_data['results'])

    # build classifiers [label_k][classifier_name][plugin_name]
    classifiers = {}
    for label_k in feature_db:
        classifiers[label_k] = {}
        for classifier_class, kwargs in CLASSIFIER_CLASSES_KWARGS:
            classifier_name = classifier_class.__name__
            if classifier_name not in classifiers[label_k]:
                classifiers[label_k][classifier_name] = {}
            for plugin_name in feature_db[label_k]:
                classifier = classifier_class(**kwargs)
                print('training '+classifier_name+' to predict '+label_k+' using '+plugin_name.replace('extractors.', ''))
                hashed_features = hasher.transform(feature_db[label_k][plugin_name]['features'])
                classifier.fit(hashed_features, feature_db[label_k][plugin_name]['labels'])
                classifiers[label_k][classifier_name][plugin_name] = classifier

    storage.store('classifiers', __name__, data=classifiers, fmt='pickle')

    global g_classifiers
    g_classifiers = classifiers


def classify(storage, hasher, file_data, train_percent, rand_seed):

    if train_percent < 100:
        # only use some files
        if file_is_for_training(file_data['fileinfo']['md5'], train_percent, rand_seed):
            # skip here, file already used in training
            print('    skipping file used in learning')
            return ''

    global g_classifiers
    if g_classifiers is None:
        g_classifiers = storage.load('classifiers', __name__)

    counters = defaultdict(Counter)

    for label_k in g_classifiers:
        print('    '+label_k)
        for classifier_name in g_classifiers[label_k]:
            for plugin_name in g_classifiers[label_k][classifier_name]:
                for plugin_data in get_plugin_data(file_data):
                    if plugin_name == plugin_data['plugin']:

                        if FEATURE_TYPE == 'string':
                            hashed_features = hasher.transform([plugin_data['results'].keys()])
                        elif FEATURE_TYPE == 'dict':
                            hashed_features = hasher.transform([plugin_data['results']])

                        # TODO: use predict_proba() instead of predict to accumulate probability for each label
                        # probabilities = g_classifiers[label_k][classifier_name][plugin_name].predict_proba(hashed_features))
                        
                        label_p = g_classifiers[label_k][classifier_name][plugin_name].predict(hashed_features)
                        try:
                            label_p = str(label_p[0])
                        except IndexError:
                            label_p = str(label_p)
                        counters[label_k][label_p] += 1
                        plugin_short_name = plugin_name.replace('extractors.', '')
                        msg = '        ' + label_p + ' according to ' + classifier_name + ' on ' + plugin_short_name
                        print(msg)

    label_results = [label_k + ':' + counter.most_common()[0][0] for label_k, counter in counters.items()]
    label_results.sort()
    conclusion = ','.join(label_results)
    print('    Conclusion: '+conclusion)
    return conclusion


def get_plugin_data(file_data):
    """
    generator to iterate over plugin data and combinations.  this gives you:
    [strings, meta, disassembly, meta+strings, disassembly+strings, disassembly+meta]
    """

    # filter out plugins without much data
    available_plugins = []
    for plugin_data in file_data['plugins']:

        # skip plugins with no results
        if len(plugin_data['results']) < 3:
            continue
        if not isinstance(plugin_data['results'], dict):
            print('Error: expecting plugin results to be a dict ' + file_data['fileinfo'])
            continue
        available_plugins.append(plugin_data)

    # yield each plugin_data by itself
    for plugin_data in available_plugins:
        yield plugin_data

    # yield merged pairs
    for pd1, pd2 in itertools.combinations(available_plugins, 2):
        names = [pd1['plugin'], pd2['plugin']]
        names.sort()
        plugin_data = {
            'plugin': '+'.join(names),
            'results': Counter(pd1['results']) + Counter(pd1['results']),
        }
        yield plugin_data
