import collections
import random
from sklearn.feature_extraction import FeatureHasher
from sklearn.ensemble import RandomForestClassifier

__version__ = '0.1'
__enabled__ = True

N_FEATURES = 24000
FEATURE_TYPE = 'string'
g_classifiers = None


def run(filepath='', fileinfo=None, storage=None, file_data=None, mode='learn', train_percent=100, rand_seed='0', **kwargs):
    # print("%s.run()" % __name__)

    # this will convert our features into a uniform format for sklearn
    hasher = FeatureHasher(n_features=N_FEATURES, input_type=FEATURE_TYPE)

    if mode == 'learn':
        return learn(storage, hasher, train_percent, rand_seed)
    else:
        return classify(storage, hasher, file_data, train_percent, rand_seed)


def file_is_for_training(filepath, train_percent, rand_seed):
    random.seed(filepath+rand_seed)
    return 100.0 * random.random() < train_percent


def learn(storage, hasher, train_percent, rand_seed):

    # load all feature data we have collected
    all_features = storage.load_all('features')

    # next we will reformat feature data and associated classifications into
    # a dict named feature_db, formatted as follows:
    # {
    #     'strings':
    #     {
    #         'classifications': ['malware', 'benign', 'benign', ... ],
    #         'features': [<hashed_features>, <hashed_features>, <hashed_features>, ...],
    #     },
    #     'meta':
    #     {
    #         'classifications': ['malware', 'benign', 'benign', ... ],
    #         'features': [<hashed_features>, <hashed_features>, <hashed_features>, ...],
    #     },
    #     ...
    # }
    feature_db = {}
    for file_data in all_features:

        if not file_data:
            continue

        if train_percent < 100:
            # only use some files
            if not file_is_for_training(file_data['filepath'], train_percent, rand_seed):
                # skip here, use this file in test instead
                continue

        # only consider files that are labeled with a classification
        if 'classification' in file_data['fileinfo']:
            if file_data['fileinfo']['classification'] is not None:

                # loop over plugins
                for plugin in file_data['plugins']:

                    # skip plugin if no results
                    if not plugin['results']:
                        continue
                    if not isinstance(plugin['results'], dict):
                        print('Error: expecting plugin results to be a dict '+file_data['fileinfo'])
                        continue

                    # build feature_db structure
                    plugin_name = plugin['plugin']
                    if plugin_name not in feature_db:
                        feature_db[plugin_name] = {}
                    if 'classifications' not in feature_db[plugin_name]:
                        feature_db[plugin_name]['classifications'] = []
                    if 'features' not in feature_db[plugin_name]:
                        feature_db[plugin_name]['features'] = []

                    # insert classification and hashed features
                    feature_db[plugin_name]['classifications'].append(file_data['fileinfo']['classification'])
                    if FEATURE_TYPE == 'string':
                        feature_db[plugin_name]['features'].append(plugin['results'].keys())
                    elif FEATURE_TYPE == 'dict':
                        feature_db[plugin_name]['features'].append(plugin['results'])

    # now build classifiers
    classifiers = {}
    for plugin_name in feature_db:
        classifier = RandomForestClassifier()
        hashed_features = hasher.transform(feature_db[plugin_name]['features'])
        classifier.fit(hashed_features, feature_db[plugin_name]['classifications'])
        print(classifier)
        classifiers[plugin_name] = classifier

    storage.store('classifiers', __name__, data=classifiers, fmt='pickle')

    global g_classifiers
    g_classifiers = classifiers


def classify(storage, hasher, file_data, train_percent, rand_seed):

    if train_percent < 100:
        # only use some files
        if file_is_for_training(file_data['filepath'], train_percent, rand_seed):
            # skip here, file already used in training
            return None

    global g_classifiers
    if g_classifiers is None:
        g_classifiers = storage.load('classifiers', __name__)

    counter = collections.Counter()
    msg = ''

    print(file_data['filepath']+'    (md5:'+file_data['fileinfo']['md5']+')')
    for plugin_name in g_classifiers:
        for plugin_data in file_data['plugins']:
            if plugin_name == plugin_data['plugin']:

                if FEATURE_TYPE == 'string':
                    hashed_features = hasher.transform([plugin_data['results'].keys()])
                elif FEATURE_TYPE == 'dict':
                    hashed_features = hasher.transform([plugin_data['results']])

                classification = g_classifiers[plugin_name].predict(hashed_features)
                try:
                    classification = str(classification[0])
                except IndexError:
                    classification = str(classification)
                counter[classification] += 1
                plugin_short_name = plugin_name.split('.')[-1]
                msg = '    ' + classification + ' according to '+g_classifiers[plugin_name].__class__.__name__ + ' on ' + plugin_short_name
                print(msg)
    final_classification = counter.most_common()[0][0]
    print('    Final classification: '+final_classification)
    return final_classification
