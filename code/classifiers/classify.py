import collections
from collections import Counter, defaultdict
import random
from sklearn.feature_extraction import FeatureHasher
from sklearn.ensemble import RandomForestClassifier

__version__ = '0.1'
__enabled__ = True

N_FEATURES = 24000
FEATURE_TYPE = 'string'
g_classifiers = None


def run(filepath='', fileinfo=None, storage=None, file_data=None, mode='learn', train_percent=100, rand_seed='0', **kwargs):
    # print("%s.run()" % __name__)

    # this will convert our features into a uniform format for sklearn
    hasher = FeatureHasher(n_features=N_FEATURES, input_type=FEATURE_TYPE)

    if mode == 'learn':
        return learn(storage, hasher, train_percent, rand_seed)
    else:
        return classify(storage, hasher, file_data, train_percent, rand_seed)


def file_is_for_training(filepath, train_percent, rand_seed):
    random.seed(filepath+rand_seed)
    return 100.0 * random.random() < train_percent


def learn(storage, hasher, train_percent, rand_seed):

    # load all feature data we have collected
    all_features = storage.load_all('features')

    # next we will reformat feature data and associated labels into
    # a dict named feature_db organized by [label][plugin] as follows:
    # {
    #     'family':
    #     {
    #         'strings':
    #         {
    #             'labels': ['family_1', 'family_2', 'family_1', ... ],
    #             'features': [<hashed_features>, <hashed_features>, <hashed_features>, ...],
    #         },
    #         'meta':
    #         {
    #             'labels': ['Fred', 'Wilma', 'Barney', ... ],
    #             'features': [<hashed_features>, <hashed_features>, <hashed_features>, ...],
    #         },
    #         'disassembly':
    #         {
    #             'labels': ['Fred', 'Wilma', 'Barney', ... ],
    #             'features': [<hashed_features>, <hashed_features>, <hashed_features>, ...],
    #         },
    #     },
    #     'author':
    #     {
    #         'strings':
    #         {
    #             ...
    #         },
    #         ...
    #     },
    #     ...
    # }
    feature_db = defaultdict(lambda: defaultdict(lambda: {'labels': [], 'features': []}))#
    for file_data in all_features:

        if not file_data:
            continue

        if train_percent < 100:
            # only use some files
            if not file_is_for_training(file_data['filepath'], train_percent, rand_seed):
                # skip here, use this file in test instead
                continue

        # only consider files that are labeled with a classification
        if 'labels' in file_data['fileinfo']:
            if file_data['fileinfo']['labels'] is not None:

                for label_k, label_v in file_data['fileinfo']['labels'].items():

                    # loop over plugins
                    for plugin in file_data['plugins']:

                        # skip plugin if no results
                        if not plugin['results']:
                            continue
                        if not isinstance(plugin['results'], dict):
                            print('Error: expecting plugin results to be a dict '+file_data['fileinfo'])
                            continue

                        # insert labels and hashed features
                        plugin_name = plugin['plugin']
                        feature_db[label_k][plugin_name]['labels'].append(label_v)
                        if FEATURE_TYPE == 'string':
                            feature_db[label_k][plugin_name]['features'].append(plugin['results'].keys())
                        elif FEATURE_TYPE == 'dict':
                            feature_db[label_k][plugin_name]['features'].append(plugin['results'])

    # now build classifiers
    classifiers = {}
    for label_k in feature_db:
        for plugin_name in feature_db[label_k]:
            classifier = RandomForestClassifier()
            hashed_features = hasher.transform(feature_db[label_k][plugin_name]['features'])
            classifier.fit(hashed_features, feature_db[label_k][plugin_name]['labels'])
            if label_k not in classifiers:
                classifiers[label_k] = {}
            classifiers[label_k][plugin_name] = classifier

    storage.store('classifiers', __name__, data=classifiers, fmt='pickle')

    global g_classifiers
    g_classifiers = classifiers


def classify(storage, hasher, file_data, train_percent, rand_seed):

    if train_percent < 100:
        # only use some files
        if file_is_for_training(file_data['filepath'], train_percent, rand_seed):
            # skip here, file already used in training
            return None

    global g_classifiers
    if g_classifiers is None:
        g_classifiers = storage.load('classifiers', __name__)

    counters = defaultdict(Counter)
    msg = ''

    print(file_data['filepath']+'    (md5:'+file_data['fileinfo']['md5']+')')
    for label_k in g_classifiers:
        for plugin_name in g_classifiers[label_k]:
            for plugin_data in file_data['plugins']:
                if plugin_name == plugin_data['plugin']:

                    if FEATURE_TYPE == 'string':
                        hashed_features = hasher.transform([plugin_data['results'].keys()])
                    elif FEATURE_TYPE == 'dict':
                        hashed_features = hasher.transform([plugin_data['results']])

                    label_p = g_classifiers[label_k][plugin_name].predict(hashed_features)
                    try:
                        label_p = str(label_p[0])
                    except IndexError:
                        label_p = str(label_p)
                    counters[label_k][label_p] += 1
                    plugin_short_name = plugin_name.split('.')[-1]
                    msg = '    ' + label_k + ' is ' + label_p + ' according to '+g_classifiers[label_k][plugin_name].__class__.__name__ + ' on ' + plugin_short_name
                    print(msg)

    conclusion = ', '.join([
        label_k + ':' + counter.most_common()[0][0] for label_k, counter in counters.items()
    ])

    print('    Conclusion: '+conclusion)
    return conclusion
